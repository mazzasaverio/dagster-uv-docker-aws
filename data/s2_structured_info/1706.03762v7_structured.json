{
  "title": "Attention Is All You Need",
  "authors": [
    "Ashish Vaswani",
    "Noam Shazeer",
    "Niki Parmar",
    "Jakob Uszkoreit",
    "Llion Jones",
    "Aidan N. Gomez",
    "≈Åukasz Kaiser",
    "Illia Polosukhin"
  ],
  "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
  "keywords": [
    "Transformer",
    "Attention mechanism",
    "Machine translation",
    "Sequence transduction",
    "Neural networks"
  ],
  "methodology": "The paper introduces the Transformer model, which relies entirely on attention mechanisms for sequence transduction tasks, eliminating the need for recurrent or convolutional layers. The model uses multi-head self-attention and feed-forward layers in both encoder and decoder stacks. Experiments were conducted on machine translation tasks using the WMT 2014 datasets, and the model was trained using the Adam optimizer with a specific learning rate schedule and regularization techniques.",
  "key_findings": [
    "The Transformer achieves state-of-the-art BLEU scores on WMT 2014 English-to-German and English-to-French translation tasks.",
    "The model is more parallelizable and requires significantly less training time compared to previous architectures.",
    "The Transformer generalizes well to other tasks, such as English constituency parsing."
  ],
  "conclusions": "The Transformer model, based entirely on attention mechanisms, outperforms previous state-of-the-art models in machine translation tasks and demonstrates the potential of attention-based architectures for various sequence transduction tasks.",
  "proposed_models": [
    "Transformer"
  ],
  "future_work": "The authors plan to extend the Transformer to handle input and output modalities other than text, investigate local attention mechanisms for large inputs and outputs, and explore less sequential generation methods."
}