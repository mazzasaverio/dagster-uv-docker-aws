=== Repository Structure ===

ðŸ“ .vscode/
  ðŸ“„ launch.json
ðŸ“ data/
ðŸ“ src/
  ðŸ“ assets/
    ðŸ“„ s1_extract_pdf_text.py
    ðŸ“„ s2_structured_info.py
    ðŸ“„ s3_db_load.py
  ðŸ“ config/
    ðŸ“„ s2_structured_info.yaml
  ðŸ“ resources/
    ðŸ“„ __init__.py
    ðŸ“„ duckdb.py
    ðŸ“„ storage.py
  ðŸ“ services/
    ðŸ“„ llm_processor.py
    ðŸ“„ structured_output_processor.py
  ðŸ“ types/
    ðŸ“„ documents.py
  ðŸ“ utils/
    ðŸ“„ config_loader.py
  ðŸ“„ __init__.py
  ðŸ“„ definitions.py
ðŸ“ terraform/
  ðŸ“„ main.tf
  ðŸ“„ outputs.tf
  ðŸ“„ variables.tf
ðŸ“„ .env
ðŸ“„ .env.example
ðŸ“„ .python-version
ðŸ“„ README.md
ðŸ“„ pyproject.toml
ðŸ“„ repository_content.txt
ðŸ“„ uv.lock


=== File Contents ===


================================================================================
File: pyproject.toml
================================================================================

[project]
name = "dagster-uv-docker-aws"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "boto3>=1.37.24",
    "dagster>=1.10.7",
    "dagster-aws>=0.26.7",
    "dagster-duckdb>=0.26.7",
    "dagster-openai>=0.26.7",
    "dagster-webserver>=1.10.7",
    "logfire>=3.12.0",
    "openai>=1.70.0",
    "pandas>=2.2.3",
    "psycopg2-binary>=2.9.10",
    "pydantic>=2.11.1",
    "unstructured[pdf]>=0.17.2",
]

[dependency-groups]
dev = [
    "pyright>=1.1.398",
    "pytest>=8.3.5",
    "ruff>=0.11.2",
]

[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[tool.dagster]
module_name = "src.definitions"
code_location_name = "src"



================================================================================
File: README.md
================================================================================

## **Overview**

Template to implement a pipeline using Dagster to extract text from PDF files, generate structured data using OpenAI's API, and store the results in a duckDB. The project is designed with scalability, modularity, and best practices in mind, making it suitable for both local development and cloud deployments.

- Quindi primo step


---

## **Features**

- **PDF Text Extraction**: Reads PDF files from local storage or S3 and extracts text using the `unstructured` library.
- **Structured Data Generation**: Processes extracted text with OpenAI to produce structured JSON data.
- **DuckDB Storage**: Store structured data in DuckDB file-based database
- **Dagster Integration**: Leverages Dagster's software-defined assets (SDAs) for modular pipeline orchestration.
- **Cloud-Ready**: Supports AWS RDS for PostgreSQL and S3 for storage.
- **Extensible Design**: Easily add new steps or modify existing ones without disrupting the pipeline.

---

## **Pipeline Workflow**

The pipeline consists of three sequential steps:

1. **PDF Text Extraction**:
    - Reads PDF files from a configurable storage backend (local filesystem or S3).
    - Extracts text using the `unstructured` library.
    - Saves the extracted text as JSON files.
2. **Structured Data Generation**:
    - Processes the extracted text with OpenAI's API.
    - Generates structured data based on a predefined schema.
    - Saves the structured data as JSON files.
3. **PostgreSQL Storage**:
    - Ingests the structured JSON files into a PostgreSQL database.
    - Creates tables dynamically based on the schema if they do not exist.

---

## **Setup Instructions**

### Prerequisites

1. Python 3.12+ installed.
3. AWS CLI configured (if using S3 or RDS).
4. Docker installed (optional for containerized deployments).



dagster dev -m src.definitions


================================================================================
File: .vscode/launch.json
================================================================================

{
    "version": "0.2.0",
    "configurations": [
      {
        "name": "Dagster",
        "type": "debugpy",
        "request": "launch",
        "module": "dagster",
        "args": [
          "dev"
        ],
        "subProcess": true,
        "pythonArgs": ["-Xfrozen_modules=off"],  // Disabilita i frozen modules
        "env": {
          "PYDEVD_DISABLE_FILE_VALIDATION": "1"  // Sopprime gli avvisi del debugger
        },
        "serverReadyAction": {
          "pattern": "Serving dagster-webserver",
          "uriFormat": "http://127.0.0.1:3000",
          "action": "openExternally"
        }
      }
    ]
  }
  


================================================================================
File: terraform/outputs.tf
================================================================================




================================================================================
File: terraform/variables.tf
================================================================================




================================================================================
File: terraform/main.tf
================================================================================




================================================================================
File: src/definitions.py
================================================================================

# src/definitions.py
from dagster import (
    Definitions,
    EnvVar,
    load_assets_from_modules,
    ScheduleDefinition,
    define_asset_job,
)
from dagster_aws.s3 import S3Resource
from src.resources.duckdb import DuckDBResource
from src.resources.storage import StorageResource, StorageType
from src.resources.openai import AzureOpenAIResource
import os

from src.assets import s1_extract_pdf_text, s2_structured_info, s3_db_load


def get_resource_defs():
    deployment_name = os.getenv("DAGSTER_DEPLOYMENT", "local")

    common_resources = {
        "duckdb": DuckDBResource(
            path=EnvVar("DUCKDB_PATH"),
        ),
        "openai": AzureOpenAIResource(
            api_key=EnvVar("AZURE_OPENAI_API_KEY"),
            endpoint=EnvVar("AZURE_OPENAI_ENDPOINT"),
            model=os.getenv("OPENAI_MODEL"),
        ),
    }

    env_specific_resources = {
        "local": {
            "storage": StorageResource(
                storage_type=StorageType.LOCAL,
                local_base_path=os.getenv("LOCAL_STORAGE_PATH", "./data"),
                s3_bucket_name=None,
            ),
        },
        "production": {
            "storage": StorageResource(
                storage_type=StorageType.S3,
                local_base_path=None,
                s3_bucket_name=EnvVar("S3_BUCKET_NAME"),
            ),
            "s3": S3Resource(
                region_name=os.getenv("AWS_REGION", "us-east-1"),
            ),
        },
    }

    return {**common_resources, **env_specific_resources[deployment_name]}


# Definizioni Dagster
defs = Definitions(
    assets=load_assets_from_modules(
        [s1_extract_pdf_text, s2_structured_info, s3_db_load]
    ),
    resources=get_resource_defs(),
    schedules=[
        ScheduleDefinition(
            name="daily_pdf_processing",
            cron_schedule="0 0 * * *",
            job=define_asset_job(
                name="daily_pdf_processing_job",
                selection="*",
            ),
        )
    ],
)



================================================================================
File: src/__init__.py
================================================================================





================================================================================
File: src/config/s2_structured_info.yaml
================================================================================

job_detail_extraction:
  model: "gpt-4o-2024-08-06" # Or your preferred model
  messages:
    - role: "system"
      content: "You are an expert in analyzing individual job posting pages. Always answer in English. Your task is to extract specific structured details from the job description content provided."
    - role: "user"
      content: "Analyze the content of this job posting page and extract the following details in a structured format. If a piece of information is not clearly present, omit the field or set it to null.\n\nPage URL: {url}\n\nPage content: {text}"
  response_format:
    type: "json_schema"
    json_schema:
      name: "job_detail_extraction"
      schema:
        type: "object"
        properties:
          description:
            type: "string"  
            description: "A brief summary or overview of the job role."
          responsibilities: 
            type: "array"
            description: "List of key responsibilities or tasks for the role."
            items:
              type: "string"
              description: "A key responsibility or task for the role."
          qualifications:
            type: "array"
            description: "List of required or preferred qualifications, skills, or experience."
            items:
              type: "string"
              description: "A required or preferred qualification, skill, or experience for the role."
          location:
            type: "string"
            description: "The primary work location(s) mentioned (e.g., 'San Francisco, CA', 'Remote', 'Hybrid - London'). Be specific."
          salary_range:
            type: "string"
            description: "Any mention of salary, compensation range, or benefits (e.g., '$100,000 - $130,000 per year', 'Competitive + Equity')."
        required: ["description", "responsibilities", "qualifications", "location", "salary_range"]
        additionalProperties: False
      strict: True 




================================================================================
File: src/resources/storage.py
================================================================================

from dagster import ConfigurableResource, get_dagster_logger
from pydantic import Field
from enum import Enum
from pathlib import Path
import os
import json
from typing import BinaryIO, Dict, List, Optional, Union, Any

logger = get_dagster_logger()


class StorageType(str, Enum):
    LOCAL = "local"
    S3 = "s3"


class StorageResource(ConfigurableResource):
    """Resource for handling file storage, either local or S3."""

    storage_type: StorageType
    local_base_path: Optional[str] = None
    s3_bucket_name: Optional[str] = None

    def validate_config(self) -> None:
        """Validate storage configuration"""
        if self.storage_type == StorageType.LOCAL and not self.local_base_path:
            raise ValueError("local_base_path must be provided for local storage")
        if self.storage_type == StorageType.S3 and not self.s3_bucket_name:
            raise ValueError("s3_bucket_name must be provided for S3 storage")

    def setup_for_execution(self, context) -> None:
        """Initialize storage"""
        self.validate_config()
        if self.storage_type == StorageType.LOCAL:
            os.makedirs(self.local_base_path, exist_ok=True)

    def list_files(self, folder_path: str, extension: str = None) -> List[str]:
        """List files in a folder with optional extension filter."""
        if self.storage_type == StorageType.LOCAL:
            base_dir = Path(self.local_base_path) / folder_path
            if not base_dir.exists():
                logger.warning(f"Directory {base_dir} does not exist.")
                return []
            files = list(base_dir.glob(f"*{extension if extension else ''}"))
            return [
                str(f.relative_to(Path(self.local_base_path)))
                for f in files
                if f.is_file()
            ]
        elif self.storage_type == StorageType.S3:
            import boto3

            s3_client = boto3.client("s3")
            try:
                response = s3_client.list_objects_v2(
                    Bucket=self.s3_bucket_name, Prefix=folder_path
                )
                if "Contents" not in response:
                    return []
                return [
                    item["Key"]
                    for item in response["Contents"]
                    if not extension or item["Key"].endswith(extension)
                ]
            except Exception as e:
                logger.error(f"Error listing S3 files: {e}")
                return []

    def read_file(self, file_path: str) -> BinaryIO:
        """Read a file from storage."""
        if self.storage_type == StorageType.LOCAL:
            full_path = Path(self.local_base_path) / file_path
            return open(full_path, "rb")
        elif self.storage_type == StorageType.S3:
            import boto3

            s3_client = boto3.client("s3")
            import io

            data = io.BytesIO()
            try:
                s3_client.download_fileobj(self.s3_bucket_name, file_path, data)
                data.seek(0)
                return data
            except Exception as e:
                logger.error(f"Error reading S3 file {file_path}: {e}")
                raise

    def write_file(
        self, file_path: str, content: Union[bytes, str, Dict[str, Any]]
    ) -> str:
        """Write content to a file in storage."""
        if self.storage_type == StorageType.LOCAL:
            full_path = Path(self.local_base_path) / file_path
            os.makedirs(full_path.parent, exist_ok=True)
            mode = "wb" if isinstance(content, bytes) else "w"
            with open(full_path, mode) as f:
                if isinstance(content, (dict, list)):
                    json.dump(content, f)
                else:
                    f.write(content)
            return str(full_path)
        elif self.storage_type == StorageType.S3:
            import boto3

            s3_client = boto3.client("s3")
            try:
                if isinstance(content, (dict, list)):
                    content = json.dumps(content).encode("utf-8")
                elif isinstance(content, str):
                    content = content.encode("utf-8")
                s3_client.put_object(
                    Bucket=self.s3_bucket_name, Key=file_path, Body=content
                )
                return f"s3://{self.s3_bucket_name}/{file_path}"
            except Exception as e:
                logger.error(f"Error writing to S3 file {file_path}: {e}")
                raise

    def read_json(self, file_path: str) -> Dict[str, Any]:
        """Read JSON file from storage."""
        if self.storage_type == StorageType.LOCAL:
            full_path = Path(self.local_base_path) / file_path
            with open(full_path) as f:
                return json.load(f)
        elif self.storage_type == StorageType.S3:
            import boto3

            s3_client = boto3.client("s3")
            try:
                response = s3_client.get_object(
                    Bucket=self.s3_bucket_name, Key=file_path
                )
                return json.loads(response["Body"].read().decode("utf-8"))
            except Exception as e:
                logger.error(f"Error reading S3 JSON file {file_path}: {e}")
                raise

    def get_full_path(self, subfolder: str) -> str:
        """Get the full path for a subfolder based on storage type."""
        if self.storage_type == StorageType.LOCAL:
            if not self.local_base_path:
                raise ValueError("local_base_path must be set for local storage")
            full_path = Path(self.local_base_path) / subfolder
            full_path.mkdir(parents=True, exist_ok=True)
            return str(full_path)
        elif self.storage_type == StorageType.S3:
            if not self.s3_bucket_name:
                raise ValueError("s3_bucket_name must be set for S3 storage")
            return f"s3://{self.s3_bucket_name}/{subfolder}"
        else:
            raise ValueError(f"Unsupported storage type: {self.storage_type}")



================================================================================
File: src/resources/duckdb.py
================================================================================

import duckdb
import os
from typing import Any, Dict, List, Optional, Union

from dagster_duckdb import DuckDBResource
import dagster as dg


logger = dg.get_dagster_logger()


class DuckDBResource(ConfigurableResource):
    """Resource for DuckDB database operations using a file-based database."""

    path: Optional[str] = None
    read_only: bool = False

    def setup_for_execution(self, context) -> None:
        """Initialize DuckDB connection using environment variable or provided path."""
        db_path = self.path or os.environ.get("DUCKDB_PATH")
        if not db_path:
            raise ValueError(
                "DuckDB path must be provided either in config or DUCKDB_PATH environment variable"
            )

        # Ensure directory exists
        os.makedirs(os.path.dirname(db_path), exist_ok=True)

        # Connect to database
        self._conn = duckdb.connect(database=db_path, read_only=self.read_only)
        logger.info(
            f"Connected to DuckDB at {db_path} ({'read-only' if self.read_only else 'read-write'})"
        )

    def create_table(self, table_name: str, schema: Dict[str, str]) -> None:
        """Create a table if it doesn't exist."""
        create_stmt = f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                {', '.join(f'{k} {v}' for k, v in schema.items())}
            );
        """
        self._conn.execute(create_stmt)
        logger.info(f"Created table {table_name} (if not exists)")

    def execute_query(self, query: str, params: Union[tuple, list, dict] = None) -> Any:
        """Execute a SQL query with optional parameters."""
        result = self._conn.execute(query, params if params else [])
        return result

    def execute_and_fetch(
        self, query: str, params: Union[tuple, list, dict] = None
    ) -> List[Dict[str, Any]]:
        """Execute a query and return results as dictionaries."""
        result = self.execute_query(query, params)
        if result.description:
            columns = [desc[0] for desc in result.description]
            return [dict(zip(columns, row)) for row in result.fetchall()]
        return []

    def execute_batch(self, query: str, data: List[tuple]) -> int:
        """Execute a batch operation with multiple parameter sets."""
        affected_rows = 0
        for params in data:
            result = self._conn.execute(query, params)
            affected_rows += result.execute_n_rows()
        return affected_rows

    def table_exists(self, table_name: str) -> bool:
        """Check if a table exists in the database."""
        result = self._conn.execute(
            f"SELECT count(*) FROM information_schema.tables WHERE table_name = '{table_name}'"
        )
        return result.fetchone()[0] > 0

    def commit(self) -> None:
        """Commit the current transaction."""
        self._conn.commit()

    def close(self) -> None:
        """Close the database connection."""
        if hasattr(self, "_conn"):
            self._conn.close()
            logger.info("Closed DuckDB connection")



================================================================================
File: src/resources/__init__.py
================================================================================




================================================================================
File: src/utils/config_loader.py
================================================================================

"""Utility module for loading configuration files."""

import os
import yaml
import logfire
from pathlib import Path
from typing import Dict, Any

# Get repository root path
repo_root = os.getenv(
    "PROJECT_ROOT",
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
)


def load_prompt_config(
    config_name: str = "job_extraction", section: str = None
) -> Dict[str, Any]:
    """Load prompt configuration from YAML file.

    Args:
        config_name (str): Name of the config file (without .yaml extension)
        section (str, optional): Section within the config to extract. If None, returns entire config.

    Returns:
        Dict[str, Any]: Configuration dictionary
    """
    config_path = Path(repo_root) / "src" / "config" / f"{config_name}.yaml"

    logfire.info(f"Loading prompt config from {config_path}")

    try:
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        if section and section in config:
            config = config[section]

        return config
    except Exception as e:
        logfire.error(
            f"Error loading config from {config_path}: {str(e)}", exc_info=True
        )
        # Return a minimal default config if file cannot be loaded
        default_config = {
            "model": "gpt-4o",
            "messages": [
                {
                    "role": "system",
                    "content": "You are an expert in analyzing job listings.",
                },
                {
                    "role": "user",
                    "content": "Extract job listings from this page: {text}",
                },
            ],
            "response_format": {"type": "json_object"},
        }
        logfire.info("Using default prompt config due to config loading failure")
        return default_config



================================================================================
File: src/services/structured_output_processor.py
================================================================================

import os
import json
import logfire
from typing import Dict, Any
from dotenv import load_dotenv
from openai import AzureOpenAI

from src.services.llm_processor import LLMProcessor

load_dotenv()


async def process_content(
    source_url: str, content_text: str, llm_config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Process job posting content using LLM to extract structured information.

    Args:
        source_url (str): The URL of the job posting
        content_text (str): The extracted text content from the job posting
        llm_config (Dict[str, Any]): Configuration for the LLM processing

    Returns:
        Dict[str, Any]: Structured job details in JSON format
    """

    logfire.info(f"Processing job content from URL: {source_url}")

    logfire.info(f"Content text: {content_text}")

    logfire.info(f"LLM config: {llm_config}")

    # Initialize Azure OpenAI client
    client = AzureOpenAI(
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        api_version="2024-12-01-preview",
    )

    llm_processor = LLMProcessor(client, llm_config)

    response_content = await llm_processor.make_request(content_text)

    job_details = json.loads(response_content)

    logfire.info(f"Successfully processed job content from {source_url}")
    return job_details



================================================================================
File: src/services/llm_processor.py
================================================================================

import json
import logfire
from typing import Dict, List, Any, Optional
from openai import AzureOpenAI
import copy
import time
import asyncio


class LLMProcessor:
    """Handles LLM requests to Azure OpenAI with configurable prompts."""

    def __init__(self, client, config_prompt):
        self.client = client
        self.config_prompt = config_prompt
        self.last_api_call_time = 0  # Timestamp dell'ultima chiamata API

        if "messages" not in self.config_prompt:
            raise ValueError("Config must contain 'messages' key")
        if "model" not in self.config_prompt:
            raise ValueError("Config must contain 'model' key")

        logfire.info(
            f"Initialized LLMProcessor with model: {self.config_prompt['model']}"
        )

    async def make_request(self, text, fields_to_extract=None):
        try:
            messages = self._prepare_messages(text)
            if not messages:
                raise ValueError("Failed to prepare messages")

            response_format = self._prepare_response_format(fields_to_extract)

            # Controllo semplice per il rate limiting
            current_time = time.time()
            time_since_last_call = current_time - self.last_api_call_time

            if time_since_last_call < 10:  # 10 secondi tra le chiamate
                wait_time = 10 - time_since_last_call
                logfire.info(f"Waiting {wait_time:.2f} seconds before next API call")
                await asyncio.sleep(wait_time)

            # Aggiorna il timestamp prima della chiamata
            self.last_api_call_time = time.time()

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                response_format=response_format,
                temperature=0,
            )

            return response.choices[0].message.content

        except Exception as e:
            logfire.error(f"Error in make_request: {str(e)}", exc_info=True)
            raise

    def _prepare_messages(self, text):
        try:
            if not text:
                return None

            messages = copy.deepcopy(self.config_prompt["messages"])

            for message in messages:
                if message["role"] == "user":
                    message["content"] = message["content"].replace("{text}", text)

            return messages

        except Exception as e:
            logfire.error(f"Error in _prepare_messages: {str(e)}", exc_info=True)
            raise

    def _prepare_response_format(self, fields_to_extract=None):
        if not fields_to_extract:
            return self.config_prompt.get("response_format", {"type": "json_object"})

        response_format = copy.deepcopy(self.config_prompt["response_format"])
        schema = response_format["json_schema"]["schema"]

        schema["properties"] = {
            field: schema["properties"][field]
            for field in fields_to_extract
            if field in schema["properties"]
        }
        schema["required"] = [
            field for field in schema["required"] if field in fields_to_extract
        ]

        return response_format



================================================================================
File: src/types/documents.py
================================================================================

# src/types/documents.py
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum


class DocumentType(str, Enum):
    PDF = "pdf"
    TEXT = "text"
    JSON = "json"


class ExtractedDocument(BaseModel):
    """Represents a document with extracted text."""

    document_id: str
    filename: str
    document_type: DocumentType
    extraction_date: datetime = Field(default_factory=datetime.now)
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)


class StructuredDocument(BaseModel):
    """Represents a document with structured data extracted."""

    document_id: str
    extraction_date: datetime = Field(default_factory=datetime.now)
    filename: str
    structured_data: Dict[str, Any]
    processing_metadata: Dict[str, Any] = Field(default_factory=dict)


class ESGReportData(BaseModel):
    """Schema for structured ESG report data extraction."""

    company_name: str
    report_year: int
    environmental_metrics: Dict[str, Any] = Field(
        description="Environmental metrics and initiatives from the report"
    )
    social_metrics: Dict[str, Any] = Field(
        description="Social metrics and initiatives from the report"
    )
    governance_metrics: Dict[str, Any] = Field(
        description="Governance metrics and practices from the report"
    )
    sustainability_goals: List[str] = Field(
        description="Key sustainability goals mentioned in the report"
    )
    carbon_emissions: Optional[Dict[str, Any]] = Field(
        description="Carbon emissions data if available", default=None
    )
    key_initiatives: List[str] = Field(
        description="Key ESG initiatives described in the report"
    )
    executive_summary: str = Field(
        description="Brief executive summary of the ESG report"
    )



================================================================================
File: src/assets/s3_db_load.py
================================================================================

from pydantic import BaseModel
from typing import List, Dict, Any
import json
from datetime import datetime

import dagster as dg
from resources import duckdb


class DuckDBStorageConfig(Config):
    table_name: str = "documents"
    schema_mapping: Dict[str, str] = {
        "document_id": "VARCHAR",
        "filename": "VARCHAR",
        "company_name": "VARCHAR",
        "report_year": "INTEGER",
        "executive_summary": "TEXT",
        "environmental_metrics": "JSON",
        "social_metrics": "JSON",
        "governance_metrics": "JSON",
        "sustainability_goals": "JSON",
        "carbon_emissions": "JSON",
        "key_initiatives": "JSON",
        "metadata": "JSON",
    }


@dg.asset(
    compute_kind="duckdb",
    group_name="load_to_database",
    deps=["extract_esg_data"],
    code_version="v1",
)
def load_to_database(
    context: dg.AssetExecutionContext,
    config: DuckDBStorageConfig,
    extract_esg_data: Dict[str, Dict],
) -> dg.Output[Dict[str, Any]]:
    """Load structured data into DuckDB database."""
    context.log.info("Starting database load")

    try:
        # Create table if not exists
        duckdb.create_table(config.table_name, config.schema_mapping)

        # Prepare records for insertion
        records = []
        for doc_id, doc_data in extract_esg_data.items():
            # Extract the extraction_date from doc_data
            extraction_date = (
                doc_data.pop("extraction_date")
                if "extraction_date" in doc_data
                else None
            )

            # Create metadata JSON
            metadata = {
                "extraction_date": extraction_date,
                "processing_date": datetime.now().isoformat(),
            }

            # Create the record
            record = {
                "document_id": doc_id,
                **doc_data,
                "metadata": json.dumps(metadata),
            }
            records.append(record)

        # Insert records
        if records:
            # Build parameters for batch insert
            columns = config.schema_mapping.keys()
            insert_query = f"""
                INSERT INTO {config.table_name} 
                ({', '.join(columns)})
                VALUES ({', '.join(['?'] * len(columns))})
            """

            rows_inserted = duckdb.execute_batch(
                insert_query, [(record.get(k) for k in columns) for record in records]
            )

            # Commit the transaction
            duckdb.commit()

            context.log.info(
                f"Inserted {rows_inserted} records into {config.table_name}"
            )
        else:
            context.log.info("No records to insert")
            rows_inserted = 0

        return Output(
            value={"rows_inserted": rows_inserted},
            metadata={
                "rows_inserted": MetadataValue.int(rows_inserted),
                "table_name": MetadataValue.text(config.table_name),
            },
        )

    except Exception as e:
        context.log.error(f"Database error: {str(e)}")
        raise



================================================================================
File: src/assets/s2_structured_info.py
================================================================================

# src/assets/s2_structured_info.py
import dagster as dg
from typing import Dict, Any
from datetime import datetime
import json
from pathlib import Path

from services.structured_output_processor import process_content

from src.resources.storage import StorageResource
from src.utils.config_loader import load_prompt_config

logger = dg.get_dagster_logger()


class ExtractionConfig(dg.Config):
    input_folder: str = "s1_extract_pdf_text"
    output_folder: str = "s2_structured_info"


@dg.asset(
    group_name="reports",
    compute_kind="openai",
    deps=["extract_pdf_text"],
    code_version="v1",
)
async def extract_structured_info(
    context: dg.AssetExecutionContext,
    config: ExtractionConfig,
    storage: StorageResource,
    extract_pdf_text: Dict[str, Dict[str, str]],
) -> dg.Output[Dict[str, Dict]]:
    """Extract structured data from reports using Azure OpenAI."""
    context.log.info("Starting data extraction")

    output_path = storage.get_full_path(config.output_folder)
    Path(output_path).mkdir(parents=True, exist_ok=True)

    config = load_prompt_config("s2_structured_info")
    llm_config = config.get("job_detail_extraction")
    context.log.info(f"LLM config: {llm_config}")

    structured_documents = {}

    for doc_id, doc_data in extract_pdf_text.items():
        try:

            doc_id = doc_data["filename"]

            content_text = doc_data["content"]
            context.log.info(f"Processing report: {doc_id}")
            context.log.info(content_text)
            json_data = await process_content(doc_id, content_text, llm_config)

            context.log.info(f"JSON data: {json_data}")

            # Save to JSON file
            json_path = Path(output_path) / f"{Path(doc_id).stem}_structured.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)

            # Store in output dictionary
            structured_documents[doc_id] = {
                "filename": doc_id,
                "extraction_date": doc_data["extraction_date"],
                "json_data": json_data,
            }

            context.log.info(f"Successfully extracted data from {doc_id}")

        except Exception as e:
            context.log.error(
                f"Error processing report {doc_data['filename']}: {str(e)}"
            )
            continue

    return dg.Output(
        value=structured_documents,
        metadata={
            "documents_processed": len(structured_documents),
            "success_rate": (
                f"{(len(structured_documents)/len(extract_pdf_text))*100:.2f}%"
                if extract_pdf_text
                else "0%"
            ),
            "output_path": output_path,
        },
    )



================================================================================
File: src/assets/s1_extract_pdf_text.py
================================================================================

from dagster import (
    asset,
    Output,
    MetadataValue,
    get_dagster_logger,
    Config,
    AssetExecutionContext,
)
from typing import Dict, Any
from pathlib import Path
import uuid
from unstructured.partition.pdf import partition_pdf
from datetime import datetime
import json

from src.resources.storage import StorageResource
from src.types.documents import ExtractedDocument, DocumentType

logger = get_dagster_logger()


class PDFExtractionConfig(Config):
    input_folder: str = "raw"
    output_folder: str = "s1_extract_pdf_text"
    batch_size: int = 10


@asset(
    compute_kind="pdf_extraction",
    group_name="documents",
    code_version="v1",
)
def extract_pdf_text(
    context: AssetExecutionContext,
    config: PDFExtractionConfig,
    storage: StorageResource,
) -> Output[Dict[str, Dict[str, str]]]:
    """Extract text from PDF files."""
    context.log.info("Starting PDF text extraction")

    input_path = storage.get_full_path(config.input_folder)
    output_path = storage.get_full_path(config.output_folder)
    context.log.info(f"Looking for PDFs in: {input_path}")
    context.log.info(f"Will save JSONs in: {output_path}")

    # Ensure output directory exists
    Path(output_path).mkdir(parents=True, exist_ok=True)

    pdf_files = list(Path(input_path).glob("**/*.pdf"))
    context.log.info(f"Found {len(pdf_files)} PDF files: {[f.name for f in pdf_files]}")

    if not pdf_files:
        context.log.warning(f"No PDF files found in {input_path}")
        return Output(value={}, metadata={"files_processed": 0})

    extracted_texts = {}

    for pdf_file in pdf_files:
        try:
            context.log.info(f"Processing {pdf_file.name}")
            elements = partition_pdf(filename=str(pdf_file))
            text_content = "\n".join([str(el) for el in elements])

            doc_id = str(uuid.uuid4())
            doc_data = {
                "filename": str(pdf_file.name),
                "content": text_content,
                "extraction_date": datetime.now().isoformat(),
            }

            # Save to JSON file with same name as PDF
            json_path = Path(output_path) / f"{pdf_file.stem}.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(doc_data, f, ensure_ascii=False, indent=2)

            extracted_texts[doc_id] = doc_data
            context.log.info(
                f"Successfully processed {pdf_file.name} and saved to {json_path}"
            )

        except Exception as e:
            context.log.error(f"Error processing {pdf_file.name}: {str(e)}")
            context.log.exception("Full error:")
            continue

    return Output(
        value=extracted_texts,
        metadata={
            "files_processed": len(extracted_texts),
            "total_files": len(pdf_files),
            "success_rate": f"{(len(extracted_texts)/len(pdf_files))*100:.2f}%",
            "input_path": input_path,
            "output_path": output_path,
            "processed_files": [f.name for f in pdf_files],
        },
    )


